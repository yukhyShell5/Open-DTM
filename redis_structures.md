# Redis Data Structures for Distributed Brute-Force Framework

This document details the Redis data structures used by the distributed brute-force framework. Redis is employed for its speed and versatility in managing job states, task queues, and live statistics.

## 1. Job Definition Hash

This Redis Hash stores the fundamental definition and relatively static configuration for a brute-force job.

*   **Key Name Pattern:** `job:<job_id>`
    *   Example: `job:abcdef123456`
*   **Type:** Redis Hash
*   **Purpose:** To store all the initial settings and core metadata of a job. This information is primarily written once upon job creation and read by the API server and potentially by workers at the start of processing a job or chunk if needed.

*   **Fields:**
    *   `job_id`: (String) Unique identifier for the job, generated by the API server.
    *   `target_info`: (String) Information about the target. This could be a URL for web brute-forcing, an IP address and port for service attacks, or simply the hash string itself if the "target" is just the hash to crack.
        *   Example: `{"type": "sha256", "hash_value": "5e884898da28047151d0e56f8dc6292773603d0d6aabbdd62a11ef721d1542d8"}` or `{"service": "ssh", "ip": "192.168.1.101", "port": 22}`
    *   `wordlist_path`: (String) Path or URL to the full wordlist in external storage (e.g., `s3://mybucket/wordlists/rockyou.txt`, `/mnt/nfs/shared/common_passwords.lst`).
    *   `total_chunks`: (Integer) The total number of chunks this job's wordlist is divided into. Calculated by the API server upon job submission based on `chunk_definition_type` and `chunk_size`.
    *   `status`: (String) Current high-level status of the job.
        *   Values: `PENDING`, `SUBMITTED`, `PREPROCESSING`, `RUNNING`, `PAUSED`, `COMPLETED_SUCCESS`, `COMPLETED_NOTFOUND`, `FAILED`, `CANCELLED`.
    *   `chunk_definition_type`: (String) Defines how the wordlist is divided into chunks.
        *   Values: `LINE_BASED` (each chunk contains a specific number of lines) or `BYTE_RANGE` (each chunk is a specific range of bytes from the wordlist file).
    *   `chunk_size`: (Integer) Number of lines (if `LINE_BASED`) or number of bytes (if `BYTE_RANGE`) per chunk.
    *   `submitted_at`: (Timestamp) ISO 8601 timestamp of when the job was submitted by the client.
    *   `started_at`: (Timestamp) ISO 8601 timestamp of when the first worker began processing a chunk for this job.
    *   `completed_at`: (Timestamp) ISO 8601 timestamp of when the job finished (either found a result, exhausted the wordlist, or was terminated).
    *   `hash_type`: (String, Optional) Specifies the hashing algorithm to be used (e.g., `SHA256`, `MD5`, `BCRYPT`). Could be part of `target_info`.

*   **Usage:**
    *   Created by the API server when a new job is submitted.
    *   Read by the API server to display job details.
    *   Read by workers to get essential job parameters if not included in the task message.

## 2. Task Queue (Pending Chunks)

This Redis List acts as a FIFO (First-In, First-Out) queue for wordlist chunks that are ready to be processed for a specific job.

*   **Key Name Pattern:** `queue:tasks:<job_id>`
    *   Example: `queue:tasks:abcdef123456`
*   **Type:** Redis List
*   **Purpose:** To decouple task generation (by the API server) from task consumption (by workers). Ensures that workers always have a ready supply of work.

*   **Message Format (JSON String):** Each item in the list is a JSON string representing a task.
    ```json
    {
      "chunk_id": "unique_chunk_identifier_uuid_or_hash",
      "job_id": "associated_job_id",
      "chunk_index": 0, // Nth chunk for this job
      "start_specifier": 0, // e.g., line number or byte offset
      "end_specifier": 999 // e.g., line number or byte offset
    }
    ```
    *   `chunk_id`: A unique identifier for this specific chunk of work. Can be a UUID or a hash of job_id and chunk_index.
    *   `job_id`: The ID of the job this chunk belongs to.
    *   `chunk_index`: The sequential index of this chunk within the job (e.g., 0, 1, 2...).
    *   `start_specifier`: Defines the beginning of the chunk.
        *   If `chunk_definition_type` is `LINE_BASED`, this is the starting line number (0-indexed).
        *   If `chunk_definition_type` is `BYTE_RANGE`, this is the starting byte offset.
    *   `end_specifier`: Defines the end of the chunk.
        *   If `chunk_definition_type` is `LINE_BASED`, this is the ending line number (inclusive or exclusive, to be consistently defined).
        *   If `chunk_definition_type` is `BYTE_RANGE`, this is the ending byte offset.

*   **Usage:**
    *   **API Server:** Populates this list (using `LPUSH` or `RPUSH`) when a job is created or resumed. It breaks down the wordlist into chunks based on `chunk_definition_type` and `chunk_size` from the Job Definition Hash.
    *   **Workers:** Use `BRPOP` (blocking right pop) or `RPOP` (right pop) to retrieve tasks from this queue. `BRPOP` is generally preferred as it allows workers to wait efficiently for new tasks.

## 3. In-Progress Tasks Set/Sorted Set

This data structure tracks chunks that are currently being processed by workers. This is vital for visibility and for handling task failures or timeouts.

### Option A: Using Redis Set

*   **Key Name Pattern:** `set:inprogress:<job_id>`
    *   Example: `set:inprogress:abcdef123456`
*   **Type:** Redis Set
*   **Purpose:** To maintain a unique list of chunks currently assigned to workers. Allows quick checking of whether a chunk is being processed and helps in identifying tasks that might need requeueing if a worker fails silently.
*   **Member Format:** `chunk_id` (the unique identifier for the chunk, same as in the task queue message).
*   **Usage:**
    1.  When a worker successfully retrieves a task (e.g., `chunk_X`) from `queue:tasks:<job_id>`, it adds `chunk_X`'s `chunk_id` to this set using `SADD`.
    2.  When the worker completes processing `chunk_X` (successfully or unsuccessfully after retries), it removes `chunk_X`'s `chunk_id` from this set using `SREM`.
    *   A separate monitoring process or the API server can periodically check this set against worker heartbeats or task durations to identify potentially orphaned tasks (tasks in this set whose assigned worker is no longer active or tasks that have been processing for an unusually long time). Orphaned tasks can then be moved back to the `queue:tasks:<job_id>`.

### Option B: Using Redis Sorted Set (ZSET) - Recommended Alternative

*   **Key Name Pattern:** `zset:inprogress:<job_id>`
    *   Example: `zset:inprogress:abcdef123456`
*   **Type:** Redis Sorted Set
*   **Purpose:** Similar to the Set, but also stores a timestamp (as the score) indicating when the task was picked up. This makes it much easier to identify tasks that have been "in progress" for too long and might be stuck or orphaned due to worker failure.
*   **Member Format:** `chunk_id` (the unique identifier for the chunk).
*   **Score:** Unix timestamp (seconds or milliseconds) representing when the worker picked up the task.
*   **Usage:**
    1.  When a worker retrieves a task (e.g., `chunk_X`), it adds `chunk_X`'s `chunk_id` to this sorted set using `ZADD zset:inprogress:<job_id> <current_timestamp> <chunk_id>`.
    2.  When the worker completes processing `chunk_X`, it removes `chunk_X`'s `chunk_id` using `ZREM`.
*   **Benefits of ZSET:**
    *   **Easy Timeout Detection:** A separate monitoring process can efficiently query the ZSET for tasks whose score (pickup timestamp) is older than a defined timeout threshold (e.g., `ZRANGEBYSCORE zset:inprogress:<job_id> 0 <now - timeout_duration>`). These identified tasks are candidates for requeueing.
    *   This is more direct than comparing a Set with external heartbeat data.

## 4. Results Storage

This structure stores the actual found credentials or success markers for a job.

*   **Key Name Pattern:** `list:results:<job_id>`
    *   Example: `list:results:abcdef123456`
*   **Type:** Redis List (allows multiple finds for the same job if, for instance, the brute-force continues after first find, or if multiple credentials are valid). A Redis Set (`set:results:<job_id>`) could be used if only unique results are desired.
*   **Purpose:** To provide a fast, centralized place for workers to report successful brute-force attempts. The API server can then retrieve these results to display to the user.

*   **Message Format:** String representing the found credential or a JSON string with more details.
    *   Simple String: `"password123"`
    *   JSON String: `{"word": "password123", "timestamp": "2023-10-27T10:30:00Z", "chunk_id": "processed_chunk_id"}`
    *   The choice of format depends on the richness of information required for each result.

*   **Usage:**
    *   **Workers:** When a worker finds a matching password or credential, it pushes the result string to this list using `LPUSH` or `RPUSH`.
    *   **API Server:** Reads from this list (e.g., using `LRANGE list:results:<job_id> 0 -1`) when a user requests the results of a completed or ongoing job.

## 5. Job Status Hash (Live Stats)

This Redis Hash stores frequently updated statistics and the operational status for a running job. It is distinct from the `job:<job_id>` hash, which holds more static definition data.

*   **Key Name Pattern:** `job_stats:<job_id>`
    *   Example: `job_stats:abcdef123456`
*   **Type:** Redis Hash
*   **Purpose:** To provide a real-time view of a job's progress and operational state. This allows for efficient updates from multiple workers and quick reads by the API server for status reports, minimizing load on the main job definition.

*   **Fields:**
    *   `status`: (String) Current operational status of the job. This often mirrors `job:<job_id>::status` but is the primary field workers and the system check for operational decisions like pausing.
        *   Values: `PENDING`, `RUNNING`, `PAUSED`, `COMPLETED_SUCCESS`, `COMPLETED_NOTFOUND`, `FAILED`, `CANCELLING`.
    *   `chunks_processed`: (Integer) Counter for the number of chunks that have been successfully processed and completed by workers.
    *   `chunks_failed`: (Integer) Counter for chunks that failed processing (e.g., due to worker error or data issues) and were possibly requeued or marked as permanently failed.
    *   `results_found`: (Integer) Counter for the number of successful brute-force attempts (i.e., credentials found). This should correspond to the number of items in `list:results:<job_id>`.
    *   `current_cps`: (Integer, Optional) Current combined speed of all workers on this job, in checks per second.
    *   `average_cps`: (Integer, Optional) Average checks per second over the job's lifetime.
    *   `estimated_completion_time`: (Timestamp or String, Optional) Estimated time until wordlist exhaustion.
    *   `last_update_worker_id`: (String, Optional) ID of the last worker that provided an update to this status hash. Useful for debugging.
    *   `last_update_timestamp`: (Timestamp) ISO 8601 timestamp of the last time any field in this hash was updated.

*   **Usage:**
    *   **Workers:**
        *   Update `chunks_processed` using `HINCRBY job_stats:<job_id> chunks_processed 1` upon successful chunk completion.
        *   Update `chunks_failed` using `HINCRBY job_stats:<job_id> chunks_failed 1` if a chunk processing fails.
        *   Update `results_found` using `HINCRBY job_stats:<job_id> results_found 1` when a result is pushed to `list:results:<job_id>`.
        *   Update `last_update_worker_id` and `last_update_timestamp` using `HMSET` or `HSET`.
        *   May periodically update `current_cps`.
    *   **API Server:** Reads this hash (using `HGETALL`) to provide clients with real-time job status and statistics.
    *   **System Components:** May update the `status` field (e.g., API server sets to `PAUSED`; a monitoring process might set to `FAILED` if critical errors occur).

---

These Redis structures form the backbone of the framework's state management and communication, enabling efficient distribution and tracking of brute-force tasks. The choice of specific Redis commands (e.g., `BRPOP` for blocking queue reads, `HINCRBY` for atomic counter updates, `ZADD`/`ZRANGEBYSCORE` for managing time-sensitive in-progress tasks) is crucial for performance and reliability.
